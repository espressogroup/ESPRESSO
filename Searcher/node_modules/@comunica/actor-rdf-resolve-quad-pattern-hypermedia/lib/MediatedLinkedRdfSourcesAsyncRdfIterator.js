"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.MediatedLinkedRdfSourcesAsyncRdfIterator = void 0;
const readable_stream_1 = require("readable-stream");
const LinkedRdfSourcesAsyncRdfIterator_1 = require("./LinkedRdfSourcesAsyncRdfIterator");
/**
 * An quad iterator that can iterate over consecutive RDF sources
 * that are determined using the rdf-resolve-hypermedia-links bus.
 *
 * @see LinkedRdfSourcesAsyncRdfIterator
 */
class MediatedLinkedRdfSourcesAsyncRdfIterator extends LinkedRdfSourcesAsyncRdfIterator_1.LinkedRdfSourcesAsyncRdfIterator {
    constructor(cacheSize, context, forceSourceType, subject, predicate, object, graph, firstUrl, maxIterators, aggregatedStore, mediators) {
        super(cacheSize, subject, predicate, object, graph, firstUrl, maxIterators);
        this.context = context;
        this.forceSourceType = forceSourceType;
        this.mediatorDereferenceRdf = mediators.mediatorDereferenceRdf;
        this.mediatorMetadata = mediators.mediatorMetadata;
        this.mediatorMetadataExtract = mediators.mediatorMetadataExtract;
        this.mediatorRdfResolveHypermedia = mediators.mediatorRdfResolveHypermedia;
        this.mediatorRdfResolveHypermediaLinks = mediators.mediatorRdfResolveHypermediaLinks;
        this.mediatorRdfResolveHypermediaLinksQueue = mediators.mediatorRdfResolveHypermediaLinksQueue;
        this.handledUrls = { [firstUrl]: true };
        this.aggregatedStore = aggregatedStore;
    }
    // Mark the aggregated store as ended once we trigger the closing or destroying of this iterator.
    // We don't override _end, because that would mean that we have to wait
    // until the buffer of this iterator must be fully consumed, which will not always be the case.
    close() {
        this.aggregatedStore?.end();
        super.close();
    }
    destroy(cause) {
        this.aggregatedStore?.end();
        super.destroy(cause);
    }
    canStartNewIterator() {
        // Also allow sub-iterators to be started if the aggregated store has at least one running iterator.
        // We need this because there are cases where these running iterators will be consumed before this linked iterator.
        return (this.aggregatedStore && this.aggregatedStore.hasRunningIterators()) || super.canStartNewIterator();
    }
    isRunning() {
        // Same as above
        return (this.aggregatedStore && this.aggregatedStore.hasRunningIterators()) || !this.done;
    }
    shouldStoreSourcesStates() {
        return this.aggregatedStore === undefined;
    }
    getLinkQueue() {
        if (!this.linkQueue) {
            this.linkQueue = this.mediatorRdfResolveHypermediaLinksQueue
                .mediate({ firstUrl: this.firstUrl, context: this.context })
                .then(result => result.linkQueue);
        }
        return this.linkQueue;
    }
    async getSourceLinks(metadata) {
        try {
            const { links } = await this.mediatorRdfResolveHypermediaLinks.mediate({ context: this.context, metadata });
            // Filter URLs to avoid cyclic next-page loops
            return links.filter(link => {
                if (this.handledUrls[link.url]) {
                    return false;
                }
                this.handledUrls[link.url] = true;
                return true;
            });
        }
        catch {
            // No next URLs may be available, for example when we've reached the end of a Hydra next-page sequence.
            return [];
        }
    }
    async getSource(link, handledDatasets) {
        // Include context entries from link
        let context = this.context;
        if (link.context) {
            context = context.merge(link.context);
        }
        // Get the RDF representation of the given document
        let url = link.url;
        let quads;
        let metadata;
        try {
            const dereferenceRdfOutput = await this.mediatorDereferenceRdf
                .mediate({ context, url });
            url = dereferenceRdfOutput.url;
            // Determine the metadata
            const rdfMetadataOutput = await this.mediatorMetadata.mediate({ context, url, quads: dereferenceRdfOutput.data, triples: dereferenceRdfOutput.metadata?.triples });
            rdfMetadataOutput.data.on('error', () => {
                // Silence errors in the data stream,
                // as they will be emitted again in the metadata stream,
                // and will result in a promise rejection anyways.
                // If we don't do this, we end up with an unhandled error message
            });
            metadata = (await this.mediatorMetadataExtract.mediate({
                context,
                url,
                // The problem appears to be conflicting metadata keys here
                metadata: rdfMetadataOutput.metadata,
                headers: dereferenceRdfOutput.headers,
                requestTime: dereferenceRdfOutput.requestTime,
            })).metadata;
            quads = rdfMetadataOutput.data;
            // Optionally filter the resulting data
            if (link.transform) {
                quads = await link.transform(quads);
            }
        }
        catch (error) {
            // Make sure that dereference errors are only emitted once an actor really needs the read quads
            // This for example allows SPARQL endpoints that error on service description fetching to still be source-forcible
            quads = new readable_stream_1.Readable();
            quads.read = () => {
                setTimeout(() => quads.emit('error', error));
                return null;
            };
            metadata = {};
        }
        // Aggregate all discovered quads into a store.
        this.aggregatedStore?.import(quads);
        // Determine the source
        const { source, dataset } = await this.mediatorRdfResolveHypermedia.mediate({
            context,
            forceSourceType: this.forceSourceType,
            handledDatasets,
            metadata,
            quads,
            url,
        });
        if (dataset) {
            // Mark the dataset as applied
            // This is needed to make sure that things like QPF search forms are only applied once,
            // and next page links are followed after that.
            handledDatasets[dataset] = true;
        }
        return { link, source, metadata, handledDatasets };
    }
}
exports.MediatedLinkedRdfSourcesAsyncRdfIterator = MediatedLinkedRdfSourcesAsyncRdfIterator;
//# sourceMappingURL=MediatedLinkedRdfSourcesAsyncRdfIterator.js.map